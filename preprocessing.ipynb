{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkSession, import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "* combine the data with the features file, to get a header\n",
    "* do variable selection and transformation\n",
    "* spit out a final `.csv` to use for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "* look at the data via kmeans with 6 clusters?\n",
    "* compute AUC on each variable to see which are important? (after regressing on the var)\n",
    "* create variables that are combinations of other variables\n",
    "* look for outliers and then run pca before clustering?\n",
    "* run multinomial regression with elastic net? (with k-fold cv)\n",
    "* or maybe try neural nets?\n",
    "* something time series?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import pandas as pd\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"xor\") \\\n",
    "    .config(\"spark.executor.memory\", '2g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in with pandas first since the data is weirdly delimited\n",
    "pandas_train_features = pd.read_csv(\"Data/X_train.txt\", sep='\\s+',header=None)\n",
    "#conversion to spark df\n",
    "df_train_features = sqlCtx.createDataFrame(pandas_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training labels\n",
    "pandas_train_labels = pd.read_csv(\"Data/y_train.txt\", sep='\\s+',header=None)\n",
    "#conversion to spark df\n",
    "df_train_labels = sqlCtx.createDataFrame(pandas_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test features\n",
    "pandas_test_features = pd.read_csv(\"Data/X_test.txt\", sep='\\s+',header=None)\n",
    "#conversion to spark df\n",
    "df_test_features = sqlCtx.createDataFrame(pandas_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test labels\n",
    "pandas_test_labels = pd.read_csv(\"Data/y_test.txt\", sep='\\s+',header=None)\n",
    "#conversion to spark df\n",
    "df_test_labels = sqlCtx.createDataFrame(pandas_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
