{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkSession, import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To update git hub\n",
    "### git add *\n",
    "### git commit -m \"Insert you change message\"\n",
    "### git push origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "* combine the data with the features file, to get a header\n",
    "* do variable selection and transformation\n",
    "* spit out a final `.csv` to use for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "* look at the data via kmeans with 6 clusters?\n",
    "* compute AUC on each variable to see which are important? (after regressing on the var)\n",
    "* create variables that are combinations of other variables\n",
    "* look for outliers and then run pca before clustering?\n",
    "* run multinomial regression with elastic net? (with k-fold cv)\n",
    "* or maybe try neural nets?\n",
    "* something time series?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import pandas as pd\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"xor\") \\\n",
    "    .config(\"spark.executor.memory\", '2g') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '1') \\\n",
    "    .config(\"spark.driver.memory\",'1g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in with pandas first since the data is weirdly delimited\n",
    "pandas_train_features = pd.read_csv(\"Data/X_train.txt\", sep='\\s+',header=None)\n",
    "#conversion to spark df\n",
    "df_train_features = sqlCtx.createDataFrame(pandas_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training labels\n",
    "pandas_train_labels = pd.read_csv(\"Data/y_train.txt\", sep='\\s+',header=None)\n",
    "pandas_train_labels.columns=['position']\n",
    "#conversion to spark df\n",
    "df_train_labels = sqlCtx.createDataFrame(pandas_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test features\n",
    "pandas_test_features = pd.read_csv(\"Data/X_test.txt\", sep='\\s+',header=None)\n",
    "#conversion to spark df\n",
    "df_test_features = sqlCtx.createDataFrame(pandas_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test labels\n",
    "pandas_test_labels = pd.read_csv(\"Data/y_test.txt\", sep='\\s+',header=None)\n",
    "pandas_test_labels.columns=['position']\n",
    "#conversion to spark df\n",
    "df_test_labels = sqlCtx.createDataFrame(pandas_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#header\n",
    "pandas_header = pd.read_csv(\"Data/features.txt\", sep='\\s+',header=None)\n",
    "#conversion to spark df. this has an index column, so we deselect it\n",
    "df_header = sqlCtx.createDataFrame(pandas_header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading subject number in\n",
    "panda_sub_train=pd.read_csv(\"Data/subject_train.txt\",sep='\\s+',header=None)\n",
    "panda_sub_train.columns=['id']\n",
    "panda_sub_test=pd.read_csv(\"Data/subject_test.txt\",sep='\\s+',header=None)\n",
    "panda_sub_test.columns=['id']\n",
    "df_sub_train=sqlCtx.createDataFrame(panda_sub_train)\n",
    "df_sub_test=sqlCtx.createDataFrame(panda_sub_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge x feature files with subject id\n",
    "merged_train=pd.merge(panda_sub_train,pandas_train_features, right_index=True, left_index=True)\n",
    "merged_test=pd.merge(panda_sub_test,pandas_test_features, right_index=True, left_index=True)\n",
    "df_merged_train_features=sqlCtx.createDataFrame(merged_train)\n",
    "df_merged_test_features=sqlCtx.createDataFrame(merged_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge train files with subject id, testing and training sets\n",
    "#pd.concat([df_a, df_b], axis=1) example\n",
    "mergetrain=pd.concat([merged_train,pandas_train_labels],axis=1)\n",
    "mergetest=pd.concat([merged_test,pandas_test_labels],axis=1)\n",
    "combine=pd.concat([mergetrain,mergetest],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete_merge=sqlCtx.createDataFrame(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data2, test_data2)=df_complete_merge.randomSplit(seed=314,weights=[0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-11-44ddd3b1d535>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-44ddd3b1d535>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    out=testing.join(test_labels,test.index=testing_labels.index,how='outer')\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "testing=df_merged_train_features.withColumn('index',monotonically_increasing_id())\n",
    "testing_labels=df_train_labels.withColumn('index',monotonically_increasing_id())\n",
    "#out=testing.join(test_labels,test.index=testing_labels.index,how='outer')\n",
    "#merge test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in y responses for training\n",
    "pandas_y_train = pd.read_csv(\"Data/y_train.txt\",header=None)\n",
    "df_y_train = sqlCtx.createDataFrame(pandas_y_train)\n",
    "\n",
    "\n",
    "#reading in y responses for test\n",
    "pandas_y_test = pd.read_csv(\"Data/y_test.txt\",header=None)\n",
    "df_y_test = sqlCtx.createDataFrame(pandas_y_test)              #Kevin\n",
    "\n",
    "\n",
    "#is there a reason we read this in twice? this data is already in pandas_train_labels/pandas_test labels, no? -- Nathan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_train_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df_merged_train_features.select([count(when(isnan(c), c)).alias(c) for c in df_merged_train_features.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_train_features.select('0','1','2','3','4').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_train_features.select('0','1','2','3','4').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.DataFrame(index=[1], columns=['train_features_obs','train_labels_obs','test_features_obs','test_labels_obs', 'header_obs'])\n",
    "ds['train_features_obs'] = df_train_features.count()\n",
    "ds['train_labels_obs'] = df_train_labels.count()\n",
    "ds['test_features_obs'] = df_test_features.count()\n",
    "ds['test_labels_obs'] = df_test_labels.count()\n",
    "ds['header_obs']= df_header.count()\n",
    "ds=sqlCtx.createDataFrame(ds)\n",
    "ds.show()                #created data frame to show how many obs in each file\n",
    "                        #makes it easier to see under df, -Kevin\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits the header column on dashes, and creates a data frame with each of the elements\n",
    "\n",
    "split_col = F.split(df_header['1'], '-')\n",
    "df_header = df_header.withColumn('Main', split_col.getItem(0))\n",
    "df_header = df_header.withColumn('Seconded', split_col.getItem(1)) #Kevin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_header.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of unique elements in the \"main\" column of the header df\n",
    "header_summary=df_header.groupBy('Main').count()\n",
    "header_summary.count()                               #Kevin\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the unique elements in \"main\" by count (show all 24 of them)\n",
    "header_summary.orderBy('count',ascending = False).show(24)      #Kevin\n",
    "\n",
    "#should we combine angles? - nathan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate data into the multiple data frames, one for each row above\n",
    "\n",
    "#step 0: try to pull out and combine angles\n",
    "header_summary.where(col('Main').like(\"%angle%\")).show()\n",
    "\n",
    "\n",
    "#step 1: create vector with the \"group names\" as entries\n",
    "header_summary.col('Main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "header_summary.groupBy(\"Main\").count().rdd.values().histogram(20)   #Kevin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "\n",
    "header_barplot= df_header.toPandas()                    # converted to pandas since easier, RDD tricky\n",
    "header_barplot['Main'].value_counts().plot(kind='bar')  #Visualize Main Variable count\n",
    "                                                        #Kevin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing subject activity count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of activites done\n",
    "y_train_summary=df_y_train.groupBy('0').count()       #0 is column name\n",
    "y_train_summary.count()     # total activities                                                  #kevin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for class imbalance in how much each activity was done.\n",
    "y_train_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_summary_dist = y_train_summary.orderBy('count', ascending = False)\n",
    "y_train_summary_dist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column for proportion\n",
    "sum_counts = df_train_labels.count()\n",
    "y_train_summary_dist.withColumn('Frequency (%)', F.round(y_train_summary_dist[1]/sum_counts * 100, 2)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_barplot= df_y_train.toPandas()                    # converted to pandas since easier, RDD tricky\n",
    "y_train_barplot['0'].value_counts().plot(kind='bar')      #maybe nicer if we change x axis to display activity label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_summary=df_y_test.groupBy('0').count()       #0 is column name\n",
    "y_test_summary.count()     # total activities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "y_train_bar=y_train_barplot['0'].value_counts() #data read\n",
    "increment=[1,2,3,4,5,6]\n",
    "names=['laying','standing','sitting','walking','walking\\ndownstairs','walking\\nupstairs']\n",
    "matplotlib.pyplot.bar(increment, y_train_bar, align='center', alpha=0.5, color='c') #Barplot\n",
    "matplotlib.pyplot.xticks(increment,names)\n",
    "matplotlib.pyplot.ylabel('Counts')\n",
    "matplotlib.pyplot.xlabel('Positions')\n",
    "matplotlib.pyplot.title('Acitivity position distribution')\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "#how are the positiosn distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_summary.orderBy('count', ascending=False).show()      #ordered from biggest to smallest\n",
    "                                                            # kevin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_barplot= df_y_test.toPandas()                    # converted to pandas since easier, RDD tricky\n",
    "y_test_barplot['0'].value_counts().plot(kind='bar')     #Kevin\n",
    "Y_test_bar=y_test_barplot['0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increment=[1,2,3,4,5,6]\n",
    "names=['laying','standing','sitting','walking','walking\\ndownstairs','walking\\nupstairs']\n",
    "matplotlib.pyplot.bar(increment, Y_test_bar, align='center', alpha=0.5, color='c') #Barplot\n",
    "matplotlib.pyplot.xticks(increment,names)\n",
    "matplotlib.pyplot.ylabel('Counts')\n",
    "matplotlib.pyplot.xlabel('Positions')\n",
    "matplotlib.pyplot.title('Acitivity position distribution')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Outlier Detection, Using Interquartile Range Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df_train_features.columns]   # exclude id from features\n",
    "bounds = {} # will store lower and upper bounds for each feature              #Will use later, -Kevin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql as sparksql\n",
    "histogram = df_train_features.select('539').rdd.flatMap(lambda x: x).histogram(11)\n",
    "\n",
    "# Loading the Computed Histogram into a Pandas Dataframe for plotting\n",
    "pd.DataFrame(\n",
    "    list(zip(*histogram)), \n",
    "    columns=['bin', 'frequency']\n",
    ").set_index(\n",
    "    'bin'\n",
    ").plot(kind='bar');\n",
    "\n",
    "#539 is fBodyBodyGyroMag-meanFreq()\n",
    "#histogram for distributiono of a variable\n",
    "#kevin- uhh kind of junk rn but may be usefull later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at class imbalance in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
